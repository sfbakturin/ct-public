# Parser

## Условие

Необходимо выбрать сайт и написать программу для *краулинга* и *парсинга* его данных. Результатом выполнения программы должен быть датасет с данными с сайта.

**Краулинг** - получение и скачивание списка страниц или элементов, содержащих искомые данные.

**Парсинг** - извлечение данных из неструктурированного текста. В данном случае из HTML-кода страниц или элементов страницы.

## Сборка и запуск

В качестве сайта выбран сайт [Мир Света](https://www.msveta.ru/). Обратите внимание, в решении используются возможно уже нерабочие прокси для скачивания страниц.

```bash
python main.py # скачивание страницы с товарами и многопоточный парсинг HTML-кода
```

По умолчанию, скачиваются только первые 15 страниц с каталогом люстр.

## Структура проекта

* [**`main.py`**](main.py) — точка входа в программу.
* [**`config.py`**](config.py) — все установленные константы в отдельном файле, в том числе и список интересуемых статей.
* [**`crawler.py`**](crawler.py) — функции скачивания страниц с товарами.
* [**`item_parser.py`**](item_parser.py) — функции разбора HTML-страниц.
* [**`writer.py`**](writer.py) — thread-safe класс записи csv-файла.
* [**`requirements.txt`**](requirements.txt) — необходимые библиотеки для запуска основного решения.
